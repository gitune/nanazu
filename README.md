# Nanazu - Nano Indexer

## これは何？

静的なコンテンツのみで構成されたサイトに全文検索機能を追加するための小さなindexerと検索用frontendのサンプルです。

## 使い方

### インデックス生成

インデックス生成スクリプトはDockerとして動作させると簡単です。

1. Dockerfileのあるディレクトリで、インデックス対象としたい静的コンテンツを `public` ディレクトリの下に置く。また生成されたインデックス格納用の `index` ディレクトリも作っておく
   - なおインデックス対象としたい静的コンテンツは、URLプレフィックス毎にサブディレクトリを切って配置してください。対象サイトが1つしかないなら `public/www/` 以下に置く、などとしておくと良いです。
2. `create_index.py` スクリプト冒頭にある「除外したいパスのプレフィックス」や「URLプレフィックスのマッピング」を使いたいサイトに合わせて更新する
3. `docker build -t nano-indexer .` などとしてdocker imageをbuildする
4. `docker run --rm -it -v ./public:/app/public -v ./index:/app/index nano-indexer` などとしてインデックスを作成する

### 検索機能設置

`frontend` 以下にあるファイルとインデックスとして生成された `nnz.all.idx` ファイルを同一のディレクトリに置くと動作します。インデックスを置くディレクトリや各スクリプトの配置、UI上の微調整などは各ファイルの中を見て適宜調整してください。

## 経緯と設計

検索はGoogleカスタム検索エンジンなどに頼っている静的なコンテンツのみで構成されたサイトに、きちんとそのサイトのコンテンツを全文検索出来るような検索機能を簡単に追加する方法はないものかといろいろ調べてみたんですが、意外にピッタリ来るものが見つかりませんでした。やりたい事は、

1. サーバとしてはあくまでも静的なコンテンツのみを提供する形としたい。CGIなど、検索リクエスト毎にサーバ側で動作させるようなプロセスは使いたくない。
2. モバイル環境などでも現実的なデータ通信量や処理時間で、ある程度の規模のサイト全体のコンテンツに対して全文検索したい。
3. 事前にインデックスをある程度の時間をかけて生成しておくのはアリ。事前生成インデックス＋フロントエンドロジックだけで、上記のような要件を満たしたい。

です。検索してみるとLunr.js等近そうなものは見つかるんですが、軽く調べてみた限りでは上記要件にピッタリくる感じでは無さそうでした(調べが足りない可能性は大いにあり)。

一昔前は、こういった感じで手軽に自サイトに全文検索機能を設けたい、というとよく話題に上がるのが[Namazu](http://www.namazu.org/index.html.ja)でした。まぁNamazuは検索にCGIを使うごく一般的な検索システムなので今回の要件にピッタリくるというわけでもないのですが、「小さく実用的」という意味で参考になりそう、ということで、Geminiさん(LLM)にそのインデックス構造などをいろいろ教わっているうちに、そこから発想をもらって出来たのが今回の「Nanazu」ことNano Indexerです。Nanazu、という名前は当然Namazuにあやかり、そのごくごく小さな子孫の一つ、くらいの気持ちです。

本家のNamazuと異なり、Nanazu(ちょっとヤヤコシイので以下nnz)はかなりいろいろ簡略化しています。日本語に対する形態素解析もインデックス生成時にしか行いませんし、インデックスも日本語はbi-gram、英語は単語ベースです。ただそれでも実用上問題の無い制度で全文検索でき、データサイズや処理時間も十分実用になりそうなことを確認しました。nnzの場合静的コンテンツ+フロントエンドロジックだけで検索したいので、検索時に形態素解析しようと思うと大きな辞書のダウンロードが必要になってしまいます。それを避けるために日本語部分(というか非ASCII単語部分)についてはbi-gramでインデックスを作ることにしました。

nnzのインデックスは以下のような構成となっています。

データ | 内容
--- | ---
nnz.dict | 対象サイトに含まれる全てのASCII単語と非ASCII bi-gramのソートされたリスト。各単語・bi-gramに対応する次のポスティングリストのポインタ情報を持つ。
nnz.idx | 各単語・bi-gram毎のそれが含まれる文書IDのリスト(ポスティングリスト)
nnz.doci | 各文書ID毎の文書DBのオフセット情報とnorm情報(文書インデックス)
nnz.docd | 各文書ID毎のタイトル、URL、ディスクリプションを含む文書DB

インデックス生成時の処理は、

1. 対象サイトの静的コンテンツ(HTML)から、不要なタグを取り除いたあと可読テキストを抜き出す
2. テキストをMeCabにより形態素解析し、名詞、動詞、形容詞、副詞、感動詞のみを含むようにフィルタする
   - ちょっとした注意点として、インデックスサイズ肥大防止やゴミの混入防止のためにインデックス作成時のみMeCabによる単語の抜き出しと単語毎のbi-gram化を行っているために、検索時にもそれを意識して検索する必要があります。つまり検索時も文章として繋がった文を入力してしまうとヒットしなくなってしまうため、探したい単語毎にスペースで分かち書きする必要があります。
3. 得られた単語が全てASCII文字から構成されていれば全体を1つの単語として、非ASCII文字が含まれていれば2文字ずつに切ってbi-gramとする
4. 文書毎に含まれる単語・bi-gramからポスティングリストを生成する
5. 文書のタイトルやURL、ディスクリプション、また検索結果のソートに用いるnormを計算して文書DBとして出力する
6. 最後に全単語・bi-gramをソート、ポスティングリストへのポインタとともに出力する

という感じで、検索時の処理としては、

1. 入力された検索語をスペースで区切ったあと、それが全てASCII文字で構成されていたら全体を単語として、そうでなければbi-gramに分解する
2. nnz.dict(単語・bi-gram辞書)とnnz.idx(ポスティングリスト)を用い、入力された単語・bi-gram毎にそれらが含まれる文書IDのリストを得る
3. 全ての入力された単語・bi-gramに共通して含まれる文書IDのみを抜き出し、TF-IDF(Term Frequency-Inverse Document Frequency)アルゴリズムに基づいて計算されたスコア値により降順にソートして検索結果とする

という感じです。

上記のような検索処理を、モバイル環境でも実用的な速度・データ量で可能にするために、以下のような工夫をしています。

<dl>
    <dt>インデックスの必要な部分のみを取得するrange GETの活用</dt>
    <dd>サイズの大きくなるnnz.idx(ポスティングリスト)とnnz.docd(文書DB)は初めからrange GETで取得することを前提とした設計としています。</dd>
    <dt>明示的な圧縮とCompression Streams APIの利用</dt>
    <dd>処理の都合上一括取得を行うnnz.dict(単語・bi-gramソート済みリスト)とnnz.doci(文書DBインデックス)は、取得時間・データ量を極力減らすためにブラウザがCompression Streams APIを利用可能な場合は圧縮版を、使えない場合は非圧縮版を使えるよう両方用意しています。</dd>
    <dt>インデックスデータの不整合を防ぐための単一ファイル化</dt>
    <dd>経路上のキャッシュ等によりインデックスファイル内の不整合を防ぐため、全ファイルを連結して一つのファイルとし、全てrange GETで取得するようにしています。</dd>
</dl>

## 実際の例

約5700ページほどある自分のサイトで利用した場合、圧縮版nnz.dictが339KBくらい、圧縮版nnz.dociが88KBくらいで、インデックス全体では11.2MB程度のサイズとなりました。またAMD Ryzen 5 3550Hを1.7GHzで稼働させている自サイトでのインデックス作成時間は3分ほどでした。
